<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="机器学习," />










<meta name="description" content="集成学习是机器学习中一种常用的手段👋 本文介绍三种常用的集成学习学习方法，分别是Bagging/Boosting/Stacking 集成学习的框架是：现有一些分类器／模型，记为$\lbrace f_1(x),f_2(x),f_3(x)…\rbrace$，这些分类器／模型需要不同，相当于每个模型其实在负责自己做的好的那一部分 集成学习需要将这些不同的模型通过合适的方式整合到一起，得到一个更强的模型">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="Ensemble">
<meta property="og:url" content="https://yeeex.gitee.io/2018/10/27/Ensemble/index.html">
<meta property="og:site_name" content="Yeeex">
<meta property="og:description" content="集成学习是机器学习中一种常用的手段👋 本文介绍三种常用的集成学习学习方法，分别是Bagging/Boosting/Stacking 集成学习的框架是：现有一些分类器／模型，记为$\lbrace f_1(x),f_2(x),f_3(x)…\rbrace$，这些分类器／模型需要不同，相当于每个模型其实在负责自己做的好的那一部分 集成学习需要将这些不同的模型通过合适的方式整合到一起，得到一个更强的模型">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://yeeex.gitee.io/2018/10/27/Ensemble/e1.png">
<meta property="og:image" content="https://yeeex.gitee.io/2018/10/27/Ensemble/e2.png">
<meta property="og:image" content="https://yeeex.gitee.io/2018/10/27/Ensemble/e3.png">
<meta property="og:updated_time" content="2019-01-19T03:44:21.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Ensemble">
<meta name="twitter:description" content="集成学习是机器学习中一种常用的手段👋 本文介绍三种常用的集成学习学习方法，分别是Bagging/Boosting/Stacking 集成学习的框架是：现有一些分类器／模型，记为$\lbrace f_1(x),f_2(x),f_3(x)…\rbrace$，这些分类器／模型需要不同，相当于每个模型其实在负责自己做的好的那一部分 集成学习需要将这些不同的模型通过合适的方式整合到一起，得到一个更强的模型">
<meta name="twitter:image" content="https://yeeex.gitee.io/2018/10/27/Ensemble/e1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://yeeex.gitee.io/2018/10/27/Ensemble/"/>





  <title>Ensemble | Yeeex</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yeeex</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">愿自由且开阔</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://yeeex.gitee.io/2018/10/27/Ensemble/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yeeex">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yeeex">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Ensemble</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-27T14:48:25+08:00">
                2018-10-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>集成学习是机器学习中一种常用的手段👋</p>
<p>本文介绍三种常用的集成学习学习方法，分别是Bagging/Boosting/Stacking</p>
<p>集成学习的框架是：现有一些分类器／模型，记为$\lbrace f_1(x),f_2(x),f_3(x)…\rbrace$，这些分类器／模型需要不同，相当于每个模型其实在负责自己做的好的那一部分</p>
<p>集成学习需要将这些不同的模型通过合适的方式整合到一起，得到一个更强的模型</p>
  <a id="more"></a>
<h2 id="Recap-Bais-amp-Variance"><a href="#Recap-Bais-amp-Variance" class="headerlink" title="Recap Bais &amp; Variance"></a>Recap Bais &amp; Variance</h2><p>集成学习是在模型的方差或者偏差方面对性能进行改善。对于每一个独立的学习任务，我们首先会定义一个函数空间，比如，模型是$f = ax^3+bx^2+c$，这个就是函数空间，而针对不同的训练数据，对于这个函数空间里的模型定义，可以学习到不同的参数：$\lbrace a,b,c\rbrace$；由此，才衍生出了讨论模型方差和偏差的概念。</p>
<p>前面提到了函数空间的概念，在描述偏差和方差之前，我们首先假定，对于学习任务T，这个世界上是有一个完美的模型能够解决这个任务。那么，我们需要做的，就是穷尽我们所有的能力，进行如下两件事：</p>
<p>（1）找到这个任务的所有全部的数据（假设存在）D</p>
<p>（2）定义一个合适的函数空间</p>
<p>首先，我们是不能够找到一个学习任务的全部数据，不同的人可能找到了不同的数据集$D_1,D_2,D_3…D_n$</p>
<p>更形象的表示，每个训练出来的模型其实是我们定义的函数空间里的一个随机变量。</p>
<p>样本容量为N的训练集合本身是一个随机变量x的集合$\lbrace x_1,x_2,x_3…x_N \rbrace$</p>
<p>训练出来的模型就是以这些随机变量为输入的，得到的一个新的关于函数空间的随机变量，我们称为随机变量函数；我们只能通过评估这些随机变脸函数的期望和方差来评估我们定义的函数空间的好坏</p>
<h3 id="偏差Bais"><a href="#偏差Bais" class="headerlink" title="偏差Bais"></a>偏差Bais</h3><p>偏差：根据我们定义的函数空间，用所有可能的训练数据集训练出来的不同模型的输出的平均值 和 这个完美的模型的输出值之间的差异；=&gt; 我们定义的函数空间离这个完美的模型有多远</p>
<p>$Bias(x)^2 = (\hat f(x)-y)^2$</p>
<p>换句话说，我们无法衡量我们这个函数空间的随机变量函数的期望和标准的输出进行比较</p>
<p>偏差越大，代表我们定义的函数空间越不完美</p>
<h3 id="方差Variance"><a href="#方差Variance" class="headerlink" title="方差Variance"></a>方差Variance</h3><p>方差：不同训练数据集合训练得到的这些随机变量函数之间的差异大小</p>
<p>方差越大，代表我们定义的函数空间越复杂 =&gt;方差其实反映了随机变量函数对于数据的敏感程度，如果定义的函数空间为$f(x) = 1$，无论采用什么样的训练集合，得到的随机变量函数都是一样的， 即 这个函数空间的方差很小，但是，显然，这个模型的偏差很大</p>
<h3 id="偏差方差分解"><a href="#偏差方差分解" class="headerlink" title="偏差方差分解"></a>偏差方差分解</h3><p>期望：$ \overline f(x) = E_D(f(x;D)) $</p>
<blockquote>
<p>x表示测试样本，D表示不同的训练集，$E_D$表示算法在不同训练集上学习到的模型的期望</p>
</blockquote>
<p>方差：$Var(x) = E_D[f(x;D)-\overline f(x)^2]$</p>
<p>噪声：$\epsilon^2 = E_D[(y_D-y)^2]$</p>
<blockquote>
<p>$y_D$为测试样本x在数据集上的标记，y为其真实标记，噪声强调了模型性能的上界，即数据集中的标记决定了模型的性能，如果标记本身就是错的，模型的性能也不会太强</p>
<p>一般情况下，默认随机噪声服从标准正态分布，即期望为0</p>
</blockquote>
<p>偏差：$bias^2(x) = (\overline f(x) - y)^2$</p>
<blockquote>
<p>期望输出与真实标记之间的差别为偏差</p>
</blockquote>
<p>偏差方差分解：</p>
<p>$E(f;D) = E_D[(f(x;D)-y_D)^2] = E_D[(f(x;D)-\overline f(x)+ \overline f(x)-y_D)^2] $</p>
<p>$ = E_D[(f(x;D)-\overline f(x))^2] + E_D[(\overline f(x)-y_D)^2]+ E_D[2(f(x;D)-\overline f(x))(\overline f(x)-y_D)]$</p>
<p>$E_D[2(f(x;D)-\overline f(x))(\overline f(x)-y_D)] = 2E_D[f(x;D)\overline f(x)-f(x;D)y_D - \overline f(x)^2 + \overline f(x)y_D] = 2(\overline f(x)^2 - \overline f(x)y_D - \overline f(x)^2 + \overline f(x) y_D) = 0$</p>
<p>$E_D[f(x;D)\overline  f(x)] = \overline f(x) E_D[f(x;D)] = \overline f(x)^2$</p>
<p>$E_D[f(x;D)y_D] = y_DE_D[f(x;D)] = y_D\overline f(x) $</p>
<p>$E(f;D) =  E_D[(f(x;D)-\overline f(x))^2] + E_D[(\overline f(x)-y_D)^2] =  E_D[(f(x;D)-\overline f(x))^2] + E_D[(\overline f(x)-y+y-y_D)^2]$</p>
<p>$E_D[(\overline f(x)-y+y-y_D)^2] = E_D[(\overline f(x)-y)^2]+ E_D[(y-y_D)^2]+2E_D[(\overline f(x)-y)(y-y_D)] =  E_D[(\overline f(x)-y)^2]+ E_D[(y-y_D)^2]$</p>
<p>$E(f;D) =  E_D[(f(x;D)-\overline f(x))^2] + E_D[(\overline f(x)-y)^2]+ E_D[(y-y_D)^2]  = variance + bias^2 + error^2$</p>
<h2 id="Recap-Math"><a href="#Recap-Math" class="headerlink" title="Recap Math"></a>Recap Math</h2><p>为了方便后面的讨论，我们需要首先定义好变量含义。</p>
<p>对于集成学习的每个模型，我们将其视为基模型，每个基模型其实是自己所在的函数空间的一个函数随机变量</p>
<p>假设每个基模型的权重$\gamma = \frac{1}{m}$，方差$\sigma^2$，以及两个基模型之间的相关系数$\rho$都相等</p>
<h2 id="同质与异质"><a href="#同质与异质" class="headerlink" title="同质与异质"></a>同质与异质</h2><p>集成学习是将每个个体学习器通过一定的方式结合形成一个强学习器的过程</p>
<p>每个个体学习器可以由同一个学习算法，通过不同的训练数据得到 =&gt;这种方式是同质的</p>
<p> =&gt; 同质的每个个体学习器可以称为基学习器</p>
<p>每个个体学习器也可以由不同的学习算法得到 =&gt; 这种方式是异质的</p>
<p>=&gt; 异质的每个个体学习器一般称为组件学习器／个体学习器</p>
<h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p><strong>Bagging的方式需要用在较复杂的模型</strong></p>
<p><strong>Bagging的方式是用在同质的基学习器中</strong></p>
<p>对于复杂的模型来说，每个模型的偏差较小，但是模型会带来较大的方差；一个改进的方式就是，将这些复杂的模型集合起来，求出平均，以此降低方差；即，目标是$E[f^*] = \hat f$ </p>
<blockquote>
<p> 每一个复杂的模型按权重集合起来，即 $f^* = \sum_i^m \gamma_if_i$ </p>
</blockquote>
<p>相当于，我们将训练数据分成n份，每一份单独训练一个复杂的模型，每个复杂的模型都可以很好的拟合自己的训练样本，但是每个模型之间的差异很大，如果将这些模型求平均，得到的就是非常贴合的模型</p>
<h3 id="BootStrap-Sampling"><a href="#BootStrap-Sampling" class="headerlink" title="BootStrap Sampling"></a>BootStrap Sampling</h3><p>对训练样本采用有放回抽样，构造不同的训练样本集合，单独训练每一个复杂的模型。</p>
<p>如果训练样本集有N个训练样本，对于每个分类器有放回采样$N^{‘}$个训练样本， </p>
<blockquote>
<p>一般另$N’ = N $，有放回抽样保证抽样得到的样本与原训练集合是不同的</p>
</blockquote>
<p>在测试样本集上，每个模型输出一个预测值，模型最终输出多个预测值的平均值／投票</p>
<p><img src="/2018/10/27/Ensemble/e1.png" width="450px"></p>
<p>Bagging的集合学习模型对强学习器更有效，是每个学习器之间是没有联系的，可以独立并行的训练，如果单个模型的性能较弱，集成以后也不会带来更大的性能提升</p>
<p>当模型容易过拟合时，非常适合采用bagging的方式，比如，决策树 Decision Tree模型</p>
<h3 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h3><p>random forest是决策树模型的bagging版本</p>
<p>决策树模型非常容易过拟合（如果每个叶子节点都是一个训练样本，那么可以达到训练误差为0），所以适合用bagging的方式进行训练</p>
<p>在Random Forest模型中，采用如下的方法对每个Decision Tree构建自己的训练集合：</p>
<p>（1）采用有放回抽样，构造每个决策树的训练集合</p>
<p>（2）对于每个决策树，随机选择在可以在哪些数据特征上进行分裂</p>
<p>采用有放回抽样构造训练集，我们<strong>其实不需要</strong>额外的从抽样好的训练集中再抽取一部分构造验证结合，来保证训练得到模型的泛化能力，假设现有四个训练样本，我们需要采用有放回抽样，构造四个训练集合，训练四个分类器：</p>
<p><img src="/2018/10/27/Ensemble/e2.png" width="350px"></p>
<p>以$x^1$为例，只有$f_1$和$f_3$两个模型的训练集合中出现了这个样本，$f_2$和$f_4$没有出现这个训练样本，则$x^1$可以作为$f_2$和$f_4$的验证样本</p>
<p>从Random Forest的角度来说：</p>
<p>$RF = f_2+f_4$，可以用这个模型验证样本$x^1$</p>
<p>$RF = f_2+f_3$，可以用这个模型验证样本$x^2$</p>
<p>$RF = f_1+f_4$，可以用这个模型验证样本$x^3$</p>
<p>$RF = f_1+f_3$，可以用这个模型验证样本$x^4$</p>
<p>我们最终的训练目的，是使得Random Forest模型在这几种情况下的验证误差最小</p>
<p>这种不需要额外切分验证集合的方式 称为 <strong>Out-of-Bag Error(OOB)</strong></p>
<h3 id="Reduce-Variance"><a href="#Reduce-Variance" class="headerlink" title="Reduce Variance"></a>Reduce Variance</h3><p>为什么bagging这种方式，对于复杂模型更有效？</p>
<p>我们bagging得到的总的模型是$f^* = \sum_i^m \gamma_if_i$ </p>
<p><strong>期望计算推导</strong></p>
<p>$E(f^*) = E(\sum_i^m\gamma_if_i) = \sum_i^m\gamma_iE(f_i)$</p>
<p>因为每个函数是从同一个函数空间的不同随机变量函数，而每一个函数的训练数据又不是相互独立的；因此，bagging中的每个基学习器可以视为具有相似的期望$\mu$和方差$\sigma^2$</p>
<p>因此，$E(f^*) = E(\sum_i^m\gamma_if_i) = \sum_i^m\gamma_iE(f_i) =\frac{1}{m}·m·\mu = \mu$</p>
<p><strong>方差计算推导</strong></p>
<p>$Var(f^*) = Var(\sum_i^m \gamma_i·f_i) = \sum_i^m Var(\gamma_i·f_i) + 2\sum_i^m\sum_{j \ne i }^m Cov(\gamma_i·f_i,\gamma_j·f_j) $</p>
<p>$ = \sum_i^m\gamma_i^2Var(f_i) +   2\sum_i^m\sum_{j \ne i }^m \gamma_i \gamma_j Cov(f_i,f_j)$</p>
<p>$Cov(f_i,f_j) = \rho \sqrt{Var(fi)}\sqrt{Var(f_j)} = \rho·\sigma^2$</p>
<p>$\sum_i^m \sum_{j \neq i}^m = \frac{m(m-1)}{2}$  =&gt; 含义是 现在的m个样本，两两之间计算相关系数，共有多少种配对</p>
<p>$Var(f^*) = m·\gamma^2·\sigma^2 + 2·\frac{m(m-1)}{2}·\gamma^2·\rho·\sigma^2 = m^2\gamma^2\sigma^2\rho + m\sigma^2\gamma^2 (1-\rho)$</p>
<p>因为每个模型的权重，我们认为是相同的 $\rho = \frac{1}{m}$，继续简化上式可得：</p>
<p>$Var(f^*) = m^2(\frac{1}{m})^2\sigma^2\rho + m(\frac{1}{m})^2\sigma^2(1-\rho) = \rho \sigma^2 + \frac{1-\rho}{m}\sigma^2$</p>
<p>💡从上面的计算可知，集成之后的总模型的期望和原有模型的期望相差不大，即，如果模型本身的偏差较大，则集成学习以后，总模型的偏差仍然较大</p>
<p>💡当各个随机变量函数之间的相关性$\rho = 1$时，集成学习后的总模型和各个基模型的方差是相等；（即如果各个基模型之间是强相关的）；一般情况下，$0&lt;\rho&lt;1 $, 因此，随着基模型的个数增加（m增加），总模型的方差是越来越小的</p>
<p>💡总模型的方差有一个极限，当模型个数趋近于无穷∞，$Var(f^*) = \rho\sigma^2$，即集成学习优化性能的极限值</p>
<p>💡而在Random Forest中，随着基决策树模型的个数增加，对于$Var(f^*)$的影响更大的是第一项$\rho\sigma^2$；因此，在随机森林中，每棵决策树在训练时，每个分支随机选择分类属性，这样可以降低各个基模型之间的相关性$\rho$；从公式的角度，随即森林模型降低的是两项$\rho \sigma^2$和$\frac{1-\rho}{m}\sigma^2$</p>
<h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p><strong>Boosting提升的是弱学习器的性能</strong></p>
<p><strong>Boosting针对的是同质的学习算法</strong></p>
<p> 假设我们现在需要的是二分类任务，训练数据是$\lbrace(x^1,\hat y^1),(x^2,\hat y^2)…(x^N,\hat y^N) \rbrace$，其中 $\hat y ^n = 1/-1$</p>
<p>Boosting的基本模式是，先训练一个分类器$f_1(x)$，接下来训练的第二个分类器需要尽可能的提升分类器的性能；通过调整第一个分类器错误分类的样本权重，使得第二个分类器在训练时更关注这些分错的样本，进而提升分类器的整体性能</p>
<p>假设每个样本的权重为$u^n$,初始权重均为1；则算法的损失函数Loss Function，需要考虑数据样本的权重：</p>
<p>$L(f) = \sum_n u^n l(f(x^n),\hat y^n)$</p>
<p>Boosting算法的训练是顺序的</p>
<h3 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h3><p>Adaboost的想法就是基分类器$f_2(x)$需要更关注$f_1(x)$分类错误❌的样本</p>
<p>对于$f_1(x)$，分类错误 $\epsilon_1 = \frac{\sum_n u_1^n \delta(f_1(x^n)\ne\hat y^n)}{Z_1} &lt; 0.5$ </p>
<blockquote>
<p>$Z_1 = \sum_n u_1^n$</p>
<p>分类错误的样本权重之和占全部样本权重的比 为分类错误率</p>
</blockquote>
<p>💡传统意义上考虑，对于二分类问题，随机猜测类别的错误率是50%，所以，一个基模型的分类错误率如果超过50%，那么这个基分类器是没有学到什么内容的</p>
<p>我们需要根据第一个基分类器的错误率，调整数据权重，用于第二个基分类器的训练；数据权重调整后，数据的权重为$u_2^n$，满足：</p>
<p>$\frac{\sum_n u_2^n \delta(f_1(x^n)\ne\hat y^n)}{Z_2} = 0.5$</p>
<p>即，第二个基分类器的训练样本的数据权重，需要满足，其在$f_1(x)$分类错误的样本的错误误差在0.5</p>
<h4 id="Re-weight-Training-Data"><a href="#Re-weight-Training-Data" class="headerlink" title="Re-weight Training Data"></a>Re-weight Training Data</h4><p>对于$f_1(x)$分类错误的训练数据$x^n$ : $u^n_2 $ &lt;- $u_1^n · d_1$</p>
<p>对于$f_1(x)$分类正确的训练数据$x^n$ : $u^n_2 $ &lt;- $u_1^n ／ d_1$</p>
<p>现在需要讨论$d_1$的值如何确定：$d_1$是一个和$f_1(x)$分类错误率有关的一个值</p>
<p>$\epsilon_1 = \frac{\sum_n u_1^n \delta(f_1(x^n)\ne\hat y^n)}{Z_1} $</p>
<p> $Z_1 = \sum_n u_1^n$</p>
<p>$\frac{\sum_n u_2^n \delta(f_1(x^n)\ne\hat y^n)}{Z_2} = 0.5$  </p>
<p>=&gt; 分子是$f_1$分类错误的样本，其权重换成$u_2^n = u_1^n · d_1$   即 $\sum_n u_2^n \delta(f_1(x^n)\ne\hat y^n) = \sum_{f_1(x^n)\ne\hat y^n} u_1^nd_1$ </p>
<p>=&gt; 分母是$Z_2 =\sum_{f_1(x^n)\ne\hat y^n} u_1^nd_1 + \sum_{f_1(x^n)=\hat y^n} u_1^n/d_1$</p>
<p>$\frac{\sum_n\;u_2^n\; \delta(f_1(x^n)\ne\hat y^n)}{Z_2} = 0.5$   =&gt; $\frac{Z_2}{\sum_n\; u_2^n\; \delta(f_1(x^n)\;\ne\;\hat y^n)} = 2$  =&gt; $\frac{\sum_{f_1(x^n)\;\ne\hat y^n}\; u_1^nd_1 + \sum_{f_1(x^n)\;=\hat y^n} \; u_1^n/d_1}{\sum_{f_1(x^n)\;\ne\;\hat y^n}\; u_1^n d_1} = 2$  </p>
<p>=&gt; $\frac{\sum_{f_1(x^n)\ne\hat y^n}\; u_1^nd_1}{\sum_{f_1(x^n)\ne\hat y^n}\; u_1^n d_1}  +\frac{ \sum_{f_1(x^n)=\hat y^n}\; u_1^n/d_1}{\sum_{f_1(x^n)\ne\hat y^n} \;u_1^n d_1} = 2 = 1 + \frac{ \sum_{f_1(x^n)=\hat y^n}\; u_1^n/d_1}{\sum_{f_1(x^n)\ne\hat y^n} \;u_1^n d_1}$</p>
<p>因此，$\sum_{f_1(x^n)=\hat y^n} u_1^n/d_1= \sum_{f_1(x^n)\ne\hat y^n} u_1^n d_1$</p>
<p>$\frac{1}{d_1}\sum_{f_1(x^n)=\hat y^n} u_1^n  = d_1 \sum_{f_1(x^n)\ne\hat y^n} u_1^n$  =&gt; $\frac{1}{d_1}Z_1(1-\epsilon_1) = d_1Z_1\epsilon_1$</p>
<p>$\sum_{f_1(x^n)=\hat y^n} u_1^n = Z_1(1-\epsilon_1) $</p>
<p>$\sum_{f_1(x^n)\ne\hat y^n} u_1^n = Z_1\epsilon_1$</p>
<p>$d_1 = \sqrt{\frac{1-\epsilon_1}{\epsilon_1}} &gt;1$ ($\epsilon$ &lt; 0.5)</p>
<p>💡这里面有一个需要讨论的地方，为什么调整数据权重以后，需要$f_1(x)$分类错误的数据，在新的权重下，达到 错误率为0.5，而不是一个其他值？</p>
<p>我个人认为，这个数据权值的改变其实代表了需要分类错误的样本的重要性，我们可以设置权重改变后，分类错误的样本的权重之和为整体权重的一半，也可以设置，分类错误的样本的权重之和为</p>
<h4 id="Adaboost-Algorithm"><a href="#Adaboost-Algorithm" class="headerlink" title="Adaboost Algorithm"></a>Adaboost Algorithm</h4><p>给定训练数据：$\lbrace(x^1,\hat y^1,u_1^1)，…,(x^n,\hat y^n,u_1^n),….(x^N,\hat y^N,u_1^N)\rbrace$</p>
<p>其中，$\hat y =1/-1$     $u_1^n = 1$</p>
<p>我们需要训练T个基分类器：</p>
<p>For t = 1，… T:</p>
<p>（1）根据当前数据权重训练基分类器 $f_t(x)$</p>
<p>（2）对于N个训练数据：</p>
<p>a. 如果数据$x^n$被基分类器分类错误，$u_{t+1}^n = u_t^n ·d_t = u_t^n ·exp(\alpha_t)$</p>
<p>b. 如果数据$x^n$被基分类器分类正确，$u_{t+1}^n = u_t^n /d_t = u_t^n ·exp(-\alpha_t)$</p>
<p>$d_t = \sqrt\frac{1-\epsilon_t}{\epsilon_t}$</p>
<p>$\alpha_t = ln \sqrt{\frac{1-\epsilon_t}{\epsilon_t}}$</p>
<p>这样，我们可以把数据权重的更新写成一个统一的表达式：</p>
<p>$u_{t+1}^n$ &lt;- $u_t^n · exp(-\hat y^n f_t(x^n) \alpha_t)$</p>
<p>当得到全部的T个基分类器以后，我们可以采用加权的方式将T个基分类器集合起来：</p>
<p>（1）统一权重：</p>
<p>$H(x) = sign(\sum_{t=1}^T f_t(x))$</p>
<p>（2）与误差相关的权重：</p>
<p>$H(x) = sign(\sum_{t=1}^T \alpha_t f_t(x))$</p>
<p>其中，$\alpha_t = ln \sqrt{\frac{1-\epsilon_t}{\epsilon_t}}$</p>
<h4 id="Reduce-Bias"><a href="#Reduce-Bias" class="headerlink" title="Reduce Bias"></a>Reduce Bias</h4><p>与Bagging中的讨论类似，我们需要讨论，Boosting对于模型偏差的影响：</p>
<p>Boosting得到的总的模型是$f^* = \sum_i^m \gamma_if_i$ </p>
<p>对于boosting来说，每个模型之间存在强相关性$\rho =1$（因为是从统一函数空间训练模型，每个模型之间的训练样本还存在较大的联系）；因此：</p>
<p>$E(f^*) = E(\sum_i^m\gamma_if_i) = \sum_i^m\gamma_iE(f_i) $</p>
<p>$Var(f^*) = m·\gamma^2·\sigma^2 + 2·\frac{m(m-1)}{2}·\gamma^2·\rho·\sigma^2 = m^2\gamma^2\sigma^2\rho + m\sigma^2\gamma^2 (1-\rho) =m^2\gamma^2\sigma^2$</p>
<p>对于集成学习后的总模型而言，如果每个基模型都是较复杂的模型，则模型的方差较大，则集成后的总模型方差也会很大，反而会导致性能的下降；所以，boosting针对的是弱学习器</p>
<p>对于每个简单模型而言，所以每个基模型的准确度都较低，因此，随着基模型数目的增加，会带来整体集成学习模型的期望增加；但是，基模型的个数不是越多越好；因为随着基模型数目不断增多，整体模型的方差也会变大                </p>
<h3 id="Margin-Theory"><a href="#Margin-Theory" class="headerlink" title="Margin Theory"></a>Margin Theory</h3><h4 id="Error-Upperbond"><a href="#Error-Upperbond" class="headerlink" title="Error Upperbond"></a>Error Upperbond</h4><p>根据前面的推导，我们得到Adaboost的分类函数为:</p>
<p>$H(x) = sign(\sum_{t=1}^T \alpha_t f_t(x))$</p>
<p>其中，$\alpha_t = ln \sqrt{\frac{1-\epsilon_t}{\epsilon_t}}$</p>
<p>接下来讨论一个问题，当boosting的模型个数不断增多，在训练样本上达到训练误差为0；是否能带来泛化的提升？</p>
<p>集成学习的loss function $ L = \frac{1}{N} \sum_n \delta(H(x) \ne \hat y^n) = \frac{1}{N} \sum_n \delta(\hat y^n g(x^n)&lt;0) \leq \frac{1}{N}\sum_nexp(-\hat y^ng(x^n))  $</p>
<p>$g(x) = \sum_{t=1}^T \alpha_tf_t(x)$</p>
<p>$\frac{1}{N}\sum_nexp(\hat y^ng(x^n)) $是误差的上界</p>
<p>设$Z_t$ 为 训练函数 $f_t$时的数据权重之和，则$Z_{t+1} = \sum_n u_{T+1}^n$</p>
<p>根据数据权重的计算公式，$u_1^n = 1$  ；$u_{t+1}^n = u_t^n \times exp(-\hat y f_t(x^n)\alpha_t)$</p>
<p> t=T+1时刻的某一个数据权重相当于是 t=1时刻开始的数据权重累乘基分类器误差，更新到t=T+1时刻，即</p>
<p>$u_{T+1}^n = \prod_{t=1}^T exp(-\hat y^n f_t(x^n)\alpha_t)$</p>
<p>$Z_{T+1} = \sum_nu_{T+1}^n= \sum_n \prod_{t=1}^T exp(-\hat y^n f_t(x^n)\alpha_t) = \sum_nexp(-\hat y^n \sum_{t=1}^T f_t(x^n)\alpha_t)$</p>
<p>$Z_{T+1} = \sum_n exp(-\hat y^n g(x))$</p>
<p>因此，误差上界 $\frac{1}{N}\sum_nexp(\hat y^ng(x^n))  = \frac{1}{N}Z_{T+1}$，即：</p>
<p>$ L = \frac{1}{N} \sum_n \delta(H(x) \ne \hat y^n) = \frac{1}{N} \sum_n \delta(\hat y^n g(x^n)&lt;0) \leq \frac{1}{N}\sum_nexp(\hat y^ng(x^n))  = \frac{1}{N}Z_{T+1} $</p>
<p>根据$Z_t$的定义，$Z_1 = N $ （每条数据的权重都是1）：</p>
<p>$Z_t = Z_{t-1} \epsilon_{t-1}\times exp(\alpha_{t-1}) + Z_{t-1}(1-\epsilon_{t-1})\times exp(-\alpha_{t-1})$ </p>
<blockquote>
<p>$\epsilon_t$：第t个分类器的训练误差</p>
<p>$Z_{t-1}\epsilon_{t-1}$：第t-1个分类器的数据权重乘以其训练误差 = 第t-1个分类器训练错误的样本权重；同理，$Z_{t-1}(1-\epsilon_{t-1})$ = 第t-1个分类器训练正确的样本权重</p>
</blockquote>
<p>$Z_t = Z_{t-1} \epsilon_{t-1}\times exp(\alpha_{t-1}) + Z_{t-1}(1-\epsilon_{t-1})\times exp(-\alpha_{t-1}) $</p>
<p>$ = Z_{t-1} \epsilon_{t-1}\times \sqrt{\frac{1-\epsilon_{t-1}}{\epsilon_{t-1}}} + Z_{t-1}(1-\epsilon_{t-1})\times \sqrt{\frac{\epsilon_{t-1}}{1-\epsilon_{t-1}}} $ </p>
<p>$= Z_{t-1}\times 2\sqrt{\epsilon_{t-1}(1-\epsilon_{t-1})}$</p>
<p>💡因为$\epsilon_t &lt; 0.5$，所以，$Z_t$的权重一定比$Z_{t-1}$小 （当$\epsilon = 0.5$时，$\alpha_t = 1$)</p>
<p>类似地，$Z_{T+1}$   t = T+1时刻的数据权重 是 从 t=1 开始,数据权重累乘 $2\sqrt{\epsilon_{t-1}(1-\epsilon_{t-1})}$，即</p>
<p>$Z_{T+1} = N \prod_{t=1}^T 2\sqrt{\epsilon_t(1-\epsilon_t)}$</p>
<p><strong>通过证明可知，随着基模型的增多，误差上界逐渐变小</strong> </p>
<h4 id="General-Formulation-of-Boosting"><a href="#General-Formulation-of-Boosting" class="headerlink" title="General Formulation of Boosting"></a>General Formulation of Boosting</h4><p>初始化函数 $g_0(x) = 0$</p>
<p>For t = 1，… T: （一共执行T步，共找到T个基分类器）</p>
<p>找到一个函数 $f_t(x)$和权重改变量$\alpha(t)$，用来优化函数$g_{t-1}(x)$ 即 :</p>
<p>$g_{t-1}(x) = \sum_{i=1}^{t-1}\alpha_if_i(x)$</p>
<p>$g_t(x) = g_{t-1}(x)+\alpha_tf_t(x)$                     （1）</p>
<p>输出 判别函数 $H(x) = sign(g_T(x))$</p>
<p>💡对于函数$g_T(x)$来说，其优化目标同样是 最小化损失函数，即</p>
<p>Minimize $L(g) = \sum_nl(\hat y^n,g(x^n)) = \sum_n exp(-\hat y^ng(x^n))$</p>
<p>所以，如果我们现在已经有$g(x) = g_{t-1}(x)$，如何优化函数$g(x)$ =&gt; 对损失函数采用梯度下降方法</p>
<p>$g_t(x) = g_{t-1}(x)-\eta \frac{\partial L(g)}{\partial g(x)} |_{g(x) = g_{t-1}(x)}$      （2）</p>
<p>💡相当于对于损失函数L(g)来说，g(x)相当于这个函数的一个参数（或者把函数g(x)理解为一个由无穷多维组成的向量，向量的每一维是函数g(x)定义域的每个值对应的函数值g(x)）</p>
<p>对比（1）式和（2）式可知，$\alpha_tf_t(x)$应该至少与$-\eta \frac{\partial L(g)}{\partial g(x)} |_{g(x) = g_{t-1}(x)}$方向一致</p>
<p>$-\eta \frac{\partial L(g)}{\partial g(x)} |_{g(x) = g_{t-1}(x)} =-\eta\sum_nexp(-\hat y^ng_{t-1}(x^n)) (-\hat y^n) $</p>
<p>即 $f_t(x)$和$\sum_nexp(-\hat y^ng_{t-1}(x^n)) (\hat y^n)$方向一致</p>
<p>所以，为了找到合适的$f_t(x)$，我们的目标是 希望 $f_t(x)$和$\sum_nexp(-\hat y^ng_{t-1}(x^n)) (\hat y^n)$的乘积越大越好</p>
<p>即 maximize $\sum_nexp(-\hat y^ng_{t-1}(x^n)) (\hat y^n)f_t(x)$</p>
<p>💡两个函数其实看作向量，向量之间的乘积越大，表明两个向量的方向越一致</p>
<p>$\sum_nexp(-\hat y^ng_{t-1}(x^n))  = \sum_nexp(-\hat y^n\sum_{i=1}^{t-1}\alpha_if_i(x^n)) = \sum_n\prod_{i=1}^{t-1} exp(-\hat y^n \alpha_if_i(x^n)) = \sum_n u^n_t$</p>
<p>最大化前半部分是在Adaboost训练过程中，训练第t个分类器时的数据权重，那么，在这个数据权重下，希望$f_t(x)$和$\hat y^n$的乘积越来越大=&gt; 即 $f_t(x)$与$\hat y^n$的同号，且$f_t(x)$的值越大越好 =&gt; 对应的$f_t(x)$即为Adaboost训练得到的函数 （Margin Theroy）</p>
<hr>
<p>再找到合适的$f_t(x)$后，我们需要讨论设置一个合适的$\alpha_t$，使得$g_t(x) = g_{t-1}(x )+\alpha_tf_t(x)$有最好的性能</p>
<p>损失函数：$L(g) = \sum_nexp(-\hat y^n(g_{t-1}(x)+\alpha_tf_t(x))) = \sum_nexp(-\hat y^ng_{t-1}(x))exp(-\hat y^n\alpha_tf_t(x)) $ </p>
<p>$= \sum_{\hat y^n \ne f_t(x)}exp(-\hat y^ng_{t-1}(x^n))exp(\alpha_t)+\sum_{\hat y^n = f_t(x)}exp(-\hat y^ng_{t-1}(x^n))exp(-\alpha_t)$      （3）</p>
<p>$\sum_{\hat y^n \ne f_t(x)}exp(-\hat y^ng_{t-1}(x^n))$  =&gt; 第t个分类器 分类错误的数据权重之和</p>
<p>$\sum_{\hat y^n = f_t(x)}exp(-\hat y^ng_{t-1}(x^n))$  =&gt;第t个分类器 分类正确的数据权重之和</p>
<p>所以，在更多的公式推导中，为了表明数据权重之和可以反应分类误差，通常会将数据权重归一化，即初始化数据权重为$\frac{1}{N}$,之和的每个数据权重$u_t^n= \frac {weight}{Z^n}$</p>
<p>这样，第t个分类器的分类错误率即 分类错误的数据权重之和</p>
<p>基于此，我们将（3）式变形：</p>
<p>$ = \epsilon_t exp(\alpha_t) + (1-\epsilon_t)exp(-\alpha_t)$</p>
<p>$\frac{\partial L(g)}{\partial \alpha_t} = \epsilon_texp(\alpha_t)-(1-\epsilon_t)exp(-\alpha_t) = 0$</p>
<p>=&gt; $\alpha_t = \frac{1}{2}ln\sqrt{\frac{1-\epsilon_t}{\epsilon_t}}$</p>
<p>💡从梯度下降的角度来看，找到合适的$f_t(x)$的过程即为梯度下降的方向优化损失函数；系数$\alpha_t$其实就是学习率；我们总归是希望找到一个更合适的学习率，使得优化是最直接的</p>
<h4 id="Margin-Theroy"><a href="#Margin-Theroy" class="headerlink" title="Margin Theroy"></a>Margin Theroy</h4><p>前面在推导的过程中，我们也提到了Margin Theroy这件事。同时，前面也提到了，随着基分类器的增多，训练误差的上界是逐渐下降的；在传统的机器学习算法中，当我们在训练样本上得到训练误差为0，训练得到的模型可能会存在过拟合的现象；但是在Adaboost中，当训练误差达到0时，继续增加基分类器，测试误差依然会减小，即人们常说的 Adaboost算法不容易过拟合。</p>
<p>在训练过程中，我们优化的训练误差，但是其实跟关心的是测试误差（泛化误差），我们希望模型在测试数据集上的效果要好；这里，泛化误差被定义为，新输入的样本的误差期望。</p>
<p>但是，如果度量模型训练误差和泛化误差和自己爱你的关系呢？这里需要引入模型容量（capacity）的概念。</p>
<p>模型容量是指模型拟合各种函数的能力。容量低的模型无法拟合训练数据集 =&gt; 欠拟合；容量高的模型泛化能力又不好 =&gt; 过拟合；</p>
<p>比如，我们认定线性回归算法将所有的线性函数视为假设空间：$\hat y = b+ wx$</p>
<p>而广义的线性回归的假设空间包含多项式函数，而非仅有线性函数，这样就是增加了模型的容量：$\hat y = b+\sum_{i=1}^9 w_ix^i$,在这个假设空间里，关于参数$w_i$是线性的</p>
<p>在机器学习中，泛化误差边界可以总结为：</p>
<p><strong>泛化误差 &lt; 训练误差+学习算法容量项</strong></p>
<p>从这个角度来说，在训练误差一定的情况下， 随着学习算法容量的增大（在Adaboost中，当训练误差为0时，随着基分类器的增多，学习算法容量项增大），则泛化误差会变大</p>
<p>因此，针对这个问题，从集成学习的角度，提出了新的泛化误差边界：</p>
<p><strong>泛化误差 &lt; 训练Margin项+学习算法容量项</strong></p>
<p>Margin这个词在SVM中是指，空间中有正负两类点，需要一个分类界面将这两类点分开，我们寻找分类界面的过程是，希望这个界面能够让正负点离这个界面的距离越大越好，这个距离即Margin</p>
<p>Margin其实刻画了能够将这些点分类正确的信心，对于正类样本点，分类器1输出的分类值为0.1，分类2输出的分类值为1；明显看出，分类器2的Margin（分类正确的信心）更大</p>
<p>在之前的推导中，我们得到了Adaboost的误差上界：</p>
<p> $ L = \frac{1}{N} \sum_n \delta(H(x) \ne \hat y^n) = \frac{1}{N} \sum_n \delta(\hat y^n g(x^n)&lt;0) \leq \frac{1}{N}\sum_nexp(-\hat y^ng(x^n)) $</p>
<p>通过这个误差上界，我们可以得知，随着基分类器的增多，误差上界会越来越小；</p>
<p>我们将误差上界表示在下图中，图中横轴表示Margin = $\hat y g(x^n)$的值，随着Margin的增大，误差上界会越来越小；</p>
<p><img src="/2018/10/27/Ensemble/e3.png" width="350px"></p>
<p>即 基分类器的增多，会带来Margin的增大， 进而使得误差上界越来越小</p>
<p>💡从这个泛化误差的边界式中也可以得知，虽然基分类器增多，会使得训练Margin项不断减小（Margin增大，泛化误差中训练Margin项对于泛化误差的影响越来越小），但是会使得模型的学习算法容量项增大，即 基分类器的增多是有限制的🚫</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/10/24/StreamingCluster/" rel="next" title="StreamingCluster">
                <i class="fa fa-chevron-left"></i> StreamingCluster
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/11/11/LearningFeasibility/" rel="prev" title="Feasibility of Learning">
                Feasibility of Learning <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Yeeex</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">25</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tag/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Recap-Bais-amp-Variance"><span class="nav-number">1.</span> <span class="nav-text">Recap Bais &amp; Variance</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#偏差Bais"><span class="nav-number">1.1.</span> <span class="nav-text">偏差Bais</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方差Variance"><span class="nav-number">1.2.</span> <span class="nav-text">方差Variance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#偏差方差分解"><span class="nav-number">1.3.</span> <span class="nav-text">偏差方差分解</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Recap-Math"><span class="nav-number">2.</span> <span class="nav-text">Recap Math</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#同质与异质"><span class="nav-number">3.</span> <span class="nav-text">同质与异质</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bagging"><span class="nav-number">4.</span> <span class="nav-text">Bagging</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#BootStrap-Sampling"><span class="nav-number">4.1.</span> <span class="nav-text">BootStrap Sampling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Random-Forest"><span class="nav-number">4.2.</span> <span class="nav-text">Random Forest</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reduce-Variance"><span class="nav-number">4.3.</span> <span class="nav-text">Reduce Variance</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Boosting"><span class="nav-number">5.</span> <span class="nav-text">Boosting</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Adaboost"><span class="nav-number">5.1.</span> <span class="nav-text">Adaboost</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Re-weight-Training-Data"><span class="nav-number">5.1.1.</span> <span class="nav-text">Re-weight Training Data</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adaboost-Algorithm"><span class="nav-number">5.1.2.</span> <span class="nav-text">Adaboost Algorithm</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Reduce-Bias"><span class="nav-number">5.1.3.</span> <span class="nav-text">Reduce Bias</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Margin-Theory"><span class="nav-number">5.2.</span> <span class="nav-text">Margin Theory</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Error-Upperbond"><span class="nav-number">5.2.1.</span> <span class="nav-text">Error Upperbond</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#General-Formulation-of-Boosting"><span class="nav-number">5.2.2.</span> <span class="nav-text">General Formulation of Boosting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Margin-Theroy"><span class="nav-number">5.2.3.</span> <span class="nav-text">Margin Theroy</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yeeex</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
