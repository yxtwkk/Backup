<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="逻辑回归Logistic Regerssion虽然带上了回归二字，但其是一个不折不扣的分类模型，在上一篇blog中，我们推导了逻辑回归和概率分类模型的关系： （1）假设各个类别和其对应的特征维度是基于高斯分布产生的 （2）假设各个类别的高斯分布的协方差矩阵是一样的 则，在二分类问题中，GDA分类模型和逻辑回归是一致的，但是两个分类模型并不是基于同一个假设学习得到的">
<meta property="og:type" content="article">
<meta property="og:title" content="Logistic Regression">
<meta property="og:url" content="https://yeeex.gitee.io/2018/06/20/逻辑回归Logistic-Regression/index.html">
<meta property="og:site_name" content="Yeeex">
<meta property="og:description" content="逻辑回归Logistic Regerssion虽然带上了回归二字，但其是一个不折不扣的分类模型，在上一篇blog中，我们推导了逻辑回归和概率分类模型的关系： （1）假设各个类别和其对应的特征维度是基于高斯分布产生的 （2）假设各个类别的高斯分布的协方差矩阵是一样的 则，在二分类问题中，GDA分类模型和逻辑回归是一致的，但是两个分类模型并不是基于同一个假设学习得到的">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://yeeex.gitee.io/2018/06/20/逻辑回归Logistic-Regression/l1.png">
<meta property="og:image" content="https://yeeex.gitee.io/2018/06/20/逻辑回归Logistic-Regression/l2.png">
<meta property="og:image" content="https://yeeex.gitee.io/2018/06/20/逻辑回归Logistic-Regression/l3.png">
<meta property="og:image" content="https://yeeex.gitee.io/2018/06/20/逻辑回归Logistic-Regression/l4.png">
<meta property="og:image" content="https://yeeex.gitee.io/2018/06/20/逻辑回归Logistic-Regression/l5.png">
<meta property="og:image" content="https://yeeex.gitee.io/2018/06/20/逻辑回归Logistic-Regression/l6.png">
<meta property="og:image" content="https://yeeex.gitee.io/2018/06/20/逻辑回归Logistic-Regression/l7.png">
<meta property="og:image" content="https://yeeex.gitee.io/2018/06/20/逻辑回归Logistic-Regression/l8.png">
<meta property="og:image" content="https://yeeex.gitee.io/2018/06/20/逻辑回归Logistic-Regression/l9.png">
<meta property="og:image" content="https://yeeex.gitee.io/2018/06/20/逻辑回归Logistic-Regression/l10.png">
<meta property="og:image" content="https://yeeex.gitee.io/2018/06/20/逻辑回归Logistic-Regression/l11.png">
<meta property="og:updated_time" content="2019-06-09T16:00:38.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Logistic Regression">
<meta name="twitter:description" content="逻辑回归Logistic Regerssion虽然带上了回归二字，但其是一个不折不扣的分类模型，在上一篇blog中，我们推导了逻辑回归和概率分类模型的关系： （1）假设各个类别和其对应的特征维度是基于高斯分布产生的 （2）假设各个类别的高斯分布的协方差矩阵是一样的 则，在二分类问题中，GDA分类模型和逻辑回归是一致的，但是两个分类模型并不是基于同一个假设学习得到的">
<meta name="twitter:image" content="https://yeeex.gitee.io/2018/06/20/逻辑回归Logistic-Regression/l1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://yeeex.gitee.io/2018/06/20/逻辑回归Logistic-Regression/"/>





  <title>Logistic Regression | Yeeex</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yeeex</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">愿自由且开阔</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://yeeex.gitee.io/2018/06/20/逻辑回归Logistic-Regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yeeex">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yeeex">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Logistic Regression</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-20T20:37:59+08:00">
                2018-06-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>逻辑回归Logistic Regerssion虽然带上了回归二字，但其是一个不折不扣的分类模型，在上一篇blog中，我们推导了逻辑回归和概率分类模型的关系：</p>
<p>（1）假设各个类别和其对应的特征维度是基于高斯分布产生的</p>
<p>（2）假设各个类别的高斯分布的协方差矩阵是一样的</p>
<p>则，在二分类问题中，GDA分类模型和逻辑回归是一致的，但是两个分类模型并不是基于同一个假设学习得到的</p>
<a id="more"></a>
<p>$p(C_1|x) = \sigma (z)$</p>
<p>$z=wx+b$</p>
<p>$p(C_1|x) = p(C_1)*p(x|C_1)/p(x)$</p>
<script type="math/tex; mode=display">p(x) =p(C_1)*p(x|C_1)+p(C_2)*p(x|C_2)</script><p><strong>=&gt; 这里的一致并不是真的一致</strong></p>
<p>（1）GDA分类模型是生成模型，是假设样本是从一个概率分布中采样得到$p(x|C_1)$（比如，高斯分布），在基于<strong>极大似然估计</strong>的情况下，求出分布的具体参数（比如，均值和协方差矩阵），再通过似然与类别先验的乘积来判断测试样本属于的类别</p>
<p>（2）逻辑回归是判别模型，即不去估计样本的分布，只是直接通过<strong>极大似然估计</strong>去计算产生样本的最大概率的参数$\sigma (z) = \sigma(wx+b)$，即直接去估计$w和b$</p>
<p>机器学习的三个步骤是 确定函数空间Function Set -&gt; 确定判别函数好坏的依据 Goodness of a Function -&gt; 根据确定的依据找到最好的函数 Best Function，我们接下来根据这个road map对逻辑回归进行讨论</p>
<h3 id="Function-Set"><a href="#Function-Set" class="headerlink" title="Function Set"></a>Function Set</h3><p>前面提到，逻辑回归即$P_{w,b}(C_1|x)$</p>
<p>如果$P_{w,b}(C_1|x) &gt; 0.5$，则样本属于C1类</p>
<p>如果$P_{w,b}(C_1|x) &lt;0.5$，则样本属于C2类</p>
<p>因为$p(C_1|x) = \sigma (z)$，并且$z=w·x+b$，则我们可以直接通过估计参数w和b来进行学习，将学习得到的z通过sigmoid的函数，即可得到对应的概率</p>
<p>Function Set：$f_{w,b}(x) = P_{w,b}(C_1|x)$</p>
<p>更形象化的表示：</p>
<p><img src="/2018/06/20/逻辑回归Logistic-Regression/l1.png" width="400px"></p>
<hr>
<h3 id="Goodness-of-a-Function"><a href="#Goodness-of-a-Function" class="headerlink" title="Goodness of a Function"></a>Goodness of a Function</h3><p>根据最大后验概率MAP，我们可以找到从函数空间选择最好函数的准则：</p>
<p>假设有如下的训练样本</p>
<p><img src="/2018/06/20/逻辑回归Logistic-Regression/l2.png" width="400px"></p>
<p>如果每个样本的产生是独立的，则产生全部训练样本的概率为：</p>
<p>$L(w,b) = f_{w,b}(x^1) f_{w,b}(x^2) (1-f_{w,b}(x^3))··· f_{w,b}(x^N)$</p>
<p>其中，$ f_{w,b}(x^1) = p(C_1|x^1)$</p>
<p>最优的参数w和b为使得$L(w,b)$最大的参数，即极大似然估计MLE：</p>
<script type="math/tex; mode=display">w^*,b^* = arg\;\underset{w,b}{max}L(w,b)</script><p>在分类问题中，如果用C1，C2这样表征类别的话，无法推导损失函数，所以，在二分类问题中，可以将C1表征为$\hat{y}^n = 1$，将C2表征为$\hat{y}^n = 0$</p>
<p>$arg\;\underset{w,b}{max}L(w,b) = arg\;\underset{w,b}{max}\;lnL(w,b) = arg\;\underset{w,b}{min}\;-lnL(w,b)$</p>
<p>$-lnL(w,b) =-ln f_{w,b}(x^1)-ln f_{w,b}(x^2) -ln(1-f_{w,b}(x^3))···-ln f_{w,b}(x^N)$</p>
<p>$-ln f_{w,b}(x^n) = -[\hat{y}^n \;lnf(x^n) + (1-\hat{y}^n )\;ln(1-f(x^n))]$</p>
<p>=&gt;$-ln f_{w,b}(x^1) = -[1\;lnf(x^1) + 0\;ln(1-f(x^1))]$</p>
<p>=&gt; $ -ln(1-f_{w,b}(x^3)) =-[0\;lnf(x^3) + 1\;ln(1-f(x^3))] $</p>
<script type="math/tex; mode=display">-lnL(w,b) = \sum -[\hat{y}^n \;lnf_{w,b}(x^n) + (1-\hat{y}^n )\;ln(1-f_{w,b}(x^n))]</script><p>经过推导，可以得出，在二分类问题中，逻辑回归 的损失函数就是两个伯努利分布的交叉熵（<strong>cross entropy between two Bernoulli distribution</strong>）</p>
<p><img src="/2018/06/20/逻辑回归Logistic-Regression/l3.png" width="400px"></p>
<p>💡交叉熵越小，两个分布越接近；如果两个分布相同，则交叉熵为0</p>
<hr>
<h3 id="Find-the-Best-Function"><a href="#Find-the-Best-Function" class="headerlink" title="Find the Best Function"></a>Find the Best Function</h3><p>根据交叉熵损失函数，我们可以通过梯度下降的方法优化更新参数：</p>
<p>$\frac{\partial -lnL(w,b))}{\partial w_i} = \sum_{n}-[\frac{\partial \hat{y}^nlnf_{w,b}(x^n)}{\partial w_i} + \frac{\partial (1-\hat{y}^n)ln(1-f_{w,b}(x^n))}{\partial w_i}]$</p>
<p>整个求导分为两个部分：</p>
<p>（1）</p>
<p>$\frac{\partial \hat{y}^nlnf_{w,b}(x^n)}{\partial w_i} = \hat{y}^n\frac{\partial lnf_{w,b}(x^n)}{\partial w_i}  = \hat{y}^n\frac{\partial lnf_{w,b}(x^n)}{\partial z} \frac{\partial z}{\partial w_i} = \hat{y}^n(1-\sigma(z))x^n_i  =  \hat{y}^n(1-f_{w,b}(x^n))x^n_i$</p>
<p>$f_{w,b}(x) = \sigma(z) = \frac{1}{1+exp(-z)}$</p>
<p>$z=w·x+b$</p>
<p>因此，可以求得：</p>
<p>$ \frac{\partial lnf_{w,b}(x^n)}{\partial z}  =  \frac{\partial ln\sigma(z)}{\partial z}=\frac{1}{\sigma(z)} \frac{\partial \sigma(z)}{\partial z}$</p>
<p>$\frac{1}{\sigma(z)} \frac{\partial \sigma(z)}{\partial z} = \frac{1}{\sigma(z)} \sigma(z)(1- \sigma(z))=1- \sigma(z)$</p>
<p>$\frac{\partial z}{\partial w_i} =x_i$</p>
<p>（2）</p>
<p>$\frac{\partial (1-\hat{y}^n)ln(1-f_{w,b}(x^n))}{\partial w_i} = (1-\hat{y}^n)\frac{\partial ln(1-f_{w,b}(x^n))}{\partial w_i} =  (1-\hat{y}^n)\frac{\partial (ln(1-\sigma(z))}{\partial z}  \frac{\partial z}{\partial w_i} = -(1-\hat{y}^n)f_{w,b}(x^n)x^n_i$</p>
<p>$\frac{\partial (ln(1-\sigma(z))}{\partial z}  = -\frac{1}{1-\sigma(z)} \frac{\partial \sigma(z)}{\partial z} = -\frac{1}{1-\sigma(z)} \sigma(z)(1- \sigma(z)) = -\sigma(z)$</p>
<p>最终的求导结果：</p>
<p>$\frac{\partial -lnL(w,b))}{\partial w_i} = \sum_{n}-[\frac{\partial \hat{y}^nlnf_{w,b}(x^n)}{\partial w_i} + \frac{\partial (1-\hat{y}^n)ln(1-f_{w,b}(x^n))}{\partial w_i}]$ </p>
<p>$= \sum_{n}-[\hat{y}^n(1-f_{w,b}(x^n))x^n_i-(1-\hat{y}^n)f_{w,b}(x^n)x^n_i]$</p>
<p>$=\sum_{n}-[\hat{y}^n-f_{w,b}(x^n)]x^n_i$</p>
<p><strong>$w_{i+1} $&lt;=$w_i -\eta\sum_{n}-[\hat{y}^n-f_{w,b}(x^n)]x^n_i $</strong></p>
<p>更新的参数的公式表明，分类得到的结果和实际类别差别越大，参数更新的越快</p>
<hr>
<h3 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h3><p><img src="/2018/06/20/逻辑回归Logistic-Regression/l4.png" width="500px"></p>
<p>虽然逻辑回归和线性回归采用了不同的损失函数，但是参数更新的公式却是一致的</p>
<h4 id="💡思考：为什么逻辑回归不能采用平方损失函数，而采用交叉熵损失函数？"><a href="#💡思考：为什么逻辑回归不能采用平方损失函数，而采用交叉熵损失函数？" class="headerlink" title="💡思考：为什么逻辑回归不能采用平方损失函数，而采用交叉熵损失函数？"></a>💡思考：为什么逻辑回归不能采用平方损失函数，而采用交叉熵损失函数？</h4><p>如果采用平方损失函数，$L(f) = \frac{1}{2}\sum_{n}(f_{w,b}(x^n)-\hat{y}^n)^2$</p>
<p>$\frac{\partial L}{\partial w_i} = (f_{w,b}(x)-\hat{y})\frac{\partial f_{w,b}(x)}{\partial z}\frac{\partial z}{\partial w_i} $</p>
<p>$= (f_{w,b}(x)-\hat{y})f_{w,b}(x)(1-f_{w,b}(x))x_i$</p>
<p>当$\hat{y} = 1,f_{w,b}(x)=1$,表明预测分类和实际分类相同</p>
<p>则$\frac{\partial L}{\partial w_i} =0$，导数为0，不用更新参数</p>
<p>当$\hat{y} = 1,f_{w,b}(x)=0$，表明预测分类和实际分类不同</p>
<p>则$\frac{\partial L}{\partial w_i} =0$，导数仍然为0，不合理</p>
<p>同样，当$\hat{y} = 0,f_{w,b}(x)=1$时，依然会有这个问题</p>
<hr>
<h3 id="Multi-Class-Classification-多分类问题"><a href="#Multi-Class-Classification-多分类问题" class="headerlink" title="Multi-Class Classification 多分类问题"></a>Multi-Class Classification 多分类问题</h3><p>在二分类问题中，可以将两个类别对应的$\hat y$设置为0或1，但是在多分类问题中，不能将其对应的$\hat y值设置为0，1，2…；因此，在多分类问题中，对应的类别可以用one-hot向量的形式表示</p>
<p>多分类中，利用softmax函数可以实现：</p>
<p>$C_1 : \; w^1,b_1$ $z_1=w^1·x+b_1$</p>
<p>$C_ 2: \; w^2,b_2$ $z_2=w^2·x+b_2$</p>
<p>$C_ 3: \; w^3,b_3$ $z_3=w^3·x+b_3$</p>
<p>softmax函数的流程：计算z —&gt; 计算$e^z$ —&gt; 对所有类别的$\sum e^z$ —&gt;计算概率 $y_i = e^z/\sum e^z$</p>
<p><img src="/2018/06/20/逻辑回归Logistic-Regression/l5.png" width="500px"></p>
<p>多分类模型中，依然采用交叉熵的方式计算损失，并求导用梯度下降的方法更新参数：</p>
<p><img src="/2018/06/20/逻辑回归Logistic-Regression/l6.png" width="500px"></p>
<p>💡注意这里将类别转换成one-hot向量</p>
<p>💡softmax是LR在多分类问题中的变形</p>
<hr>
<h3 id="Limitation-of-LR-逻辑回归的局限性"><a href="#Limitation-of-LR-逻辑回归的局限性" class="headerlink" title="Limitation of LR 逻辑回归的局限性"></a>Limitation of LR 逻辑回归的局限性</h3><p><img src="/2018/06/20/逻辑回归Logistic-Regression/l7.png" width="500px"></p>
<p>对于如上的样本，我们是否可以通过LR将其分开？</p>
<p>LR的分类界面是一个线性模型，在二维度的二分类问题中，线性模型就是一条直线，我们无法通过一条直线将如上的两个样本分开</p>
<h4 id="特征变换-Feature-Transformation"><a href="#特征变换-Feature-Transformation" class="headerlink" title="特征变换 Feature Transformation"></a>特征变换 Feature Transformation</h4><p>在拿到训练样本时，如果样本的几个特征维度不在同一个级别的量纲内，为了使得各个维度的均衡性，需要进行特征缩放Feature Scaling。我们可以通过特征变换Feature Transformation，解决上述提到的LR无法解决的问题</p>
<p>特征缩放的具体方法是启发式的，对于上述问题，我们可以将原有的两个维度对应到如下两个维度：</p>
<p>第一维：样本到点（0，0）的距离</p>
<p>第二维：样本到点（1，1）的距离</p>
<p>这样，原有的四个训练样本，就变换到新的特征空间$x^{‘}_1\;x^{‘}_2$，形成新的三个训练样本$[1,1]\;[\sqrt{2},0]\;[0,\sqrt{2}]$</p>
<p>当然，我们通过机器学习的方式学习得到特征变换的函数：通过级联多个逻辑回归函数，将$x_1,x_2变换到</p>
<p>$x^{‘}_1\;x^{‘}_2$，然后再将其送入分类器中进行分类（图示省略了bias $b$）</p>
<p><img src="/2018/06/20/逻辑回归Logistic-Regression/l8.png" width="500px"></p>
<hr>
<h3 id="生成模型／判别模型"><a href="#生成模型／判别模型" class="headerlink" title="生成模型／判别模型"></a>生成模型／判别模型</h3><p>前文通过公式推导，得出GDA在共用一个协方差矩阵的情况下和LR是等价的，但是这种等价并不是真正的等价，二者其实是基于不同的假设得到的：</p>
<p>GDA：假设样本服从高斯分布 =&gt; <strong>生成模型 Discriminative Model</strong></p>
<p>LR：不估计样本服从的分布，只去计算样本产生的概率 =&gt; <strong>判别模型 Generative Model</strong></p>
<p>生成模型是对类条件概率密度p(x|y)和先验概率p(y)进行建模，提取出样本服从的分布，然后根据贝叶斯公式计算p(y|x)进行判断，这种模型其实是建立在无穷样本的基础上，是对样本的概率密度进行估计</p>
<p>判别模型是对后验概率p(y|x)直接进行建模，或者直接学习输入空间到输出空间的映射关系，适用于有限样本的情况下</p>
<p>💡判别模型因为其没有非常强的假设， 一般情况下，判别模型的性能是优于生成模型的性能</p>
<h4 id="以朴素贝叶斯为例，对两类模型进行说明"><a href="#以朴素贝叶斯为例，对两类模型进行说明" class="headerlink" title="以朴素贝叶斯为例，对两类模型进行说明"></a><strong>以朴素贝叶斯为例，对两类模型进行说明</strong></h4><p>假设有如下的训练样本，去分类样本（1,1）：</p>
<p><img src="/2018/06/20/逻辑回归Logistic-Regression/l9.png" width="500px"></p>
<p>从直观的角度来说，因为训练样本里已经有(1,1)这样一个训练样本，所以应该将这个样本分类为Class1，但是在朴素贝叶斯分类器中，这个样本却被分类成Class 2</p>
<p>💡朴素贝叶斯假设，各个特征维度的产生是独立的，即不考虑各个维度之间的关系！</p>
<p>根据训练样本，得到参数：</p>
<p><img src="/2018/06/20/逻辑回归Logistic-Regression/l10.png" width="500px"></p>
<p>则根据以上估计，测试样本被分类为Class 1的概率小于0.5：</p>
<p><img src="/2018/06/20/逻辑回归Logistic-Regression/l11.png" width="400px"></p>
<h4 id="讨论-Generative-v-s-Discriminative"><a href="#讨论-Generative-v-s-Discriminative" class="headerlink" title="讨论 Generative v.s. Discriminative"></a>讨论 Generative v.s. Discriminative</h4><p>（1）判别模型的性能好坏是与训练样本量正相关，而生成模型往往因为其有很强的假设，数据量对其的影响没有对判别模型的影响大</p>
<p>（2）因为生成模型有很强的假设，所以对噪声的鲁棒性更好</p>
<p>（3）生成模型中的先验概率和条件似然可以从不同的数据源中估计</p>
<hr>
<h3 id="极大似然估计MLE-最大后验概率MAP"><a href="#极大似然估计MLE-最大后验概率MAP" class="headerlink" title="极大似然估计MLE /最大后验概率MAP"></a>极大似然估计MLE /最大后验概率MAP</h3><p>最大似然估计MLE：<strong>模型已定，参数未知</strong></p>
<p>对于高斯判别分析GDA，假定样本服从正态分布，但是不知道均值和协方差矩阵</p>
<p>对于逻辑回归LR，假定样本服从二项分布，但是不知道均值</p>
<p>最大后验概率MAP：假设在MLE中用来估计参数$\mu$,但是MLE不考虑$\mu$的先验概率，但是如果考虑$\mu$服从某个先验概率，则需进行最大后验概率估计MAP</p>
<hr>
<h3 id="交叉熵与平方损失的不同"><a href="#交叉熵与平方损失的不同" class="headerlink" title="交叉熵与平方损失的不同"></a>交叉熵与平方损失的不同</h3><p>在SVM讨论损失函数中，讨论了在采用sigmoid函数的情况，用平方损失和交叉熵损失的不同</p>
<p>平方损失$L(\sigma(z),y) = (y·\sigma(z) - 1)^2​$</p>
<p>交叉熵损失$L(\sigma(z),y) = ln(1+exp(-y\sigma(z)))​$</p>
<p>根据二者的函数图像，可以得到，当$y\sigma(z)​$的乘积小于0 且绝对值较大时，交叉熵损失的梯度较大，可以更好的进行训练</p>
<p>对于深度神经网络中，我们在最后一层通常使用平方损失／交叉损失：</p>
<p>平方损失更适合输出为连续，且最后一层不含sigmoid或softmax的神经网络；交叉熵适合二分类和多分类的场景。</p>
<p>我们以神经网络最后一层为例，假设输入激活函数为z，从激活函数输出为a，我们利用a和实际值y进行比较，计算损失；因为神经网络存在误差的反向传播，我们可以先从最后一层计算误差关于z的导数，这样可以继续向后传播。</p>
<p>平方损失：$L(a,y) = \frac{1}{2}||y-a||^2 = \frac{1}{2}||y-f(z)||^2$  （其中，$f(z)$是关于z的激活函数）</p>
<p>$\frac{\partial L}{\partial z} = -(y-a)f’(z)​$</p>
<p>交叉熵损失：$L(a,y) = -\sum_{k=1}^n y_klna_k = -\sum_{k=1}^ny_klnf(z_k)$</p>
<p>$\frac{\partial L}{\partial z} = -\frac{1}{a_k}·\frac{\partial a_k}{\partial z_k} ​$</p>
<p>如果激活函数取SoftMax激活函数，则根据SoftMax函数的计算公式，我们需要讨论两种情况：</p>
<p>$a = \frac{z_1}{z_1+z_2+z_3}$</p>
<p>如果此时反传的误差与$z_1$有关，则$\frac{\partial a}{\partial z_1} = \frac{z_1’}{z_1+z_2+z_3} - \frac{z_1}{(z_1+z_2+z_3)^2} = a(1-a)$</p>
<p>${z_1}’ = {e^{z}}’ = e^z$</p>
<p>如果此时反传的误差与$z_1$无关，假设是$z_2$，则$\frac{\partial a’}{\partial z_2} = \frac{0}{z_1+z_2+z_3} - \frac{z_1· (z_2)’}{(z_1+z_2+z_3)^2} = -a_1·a_2$ </p>
<p>因此，我们可以看出，采用平方损失函数，当激活函数为sigmoid时，如果z的绝对值很大，$f’(z)$会趋近于饱和，</p>
<p>$f’(z) = f(z)(1-f(z))​$， 导致梯度非常小，会使得学习变得比较缓慢</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/06/16/分类Classification/" rel="next" title="Classification">
                <i class="fa fa-chevron-left"></i> Classification
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/06/26/深度学习DeepLearning介绍/" rel="prev" title="DeepLearning Introduction">
                DeepLearning Introduction <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Yeeex</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">32</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Function-Set"><span class="nav-number">1.</span> <span class="nav-text">Function Set</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Goodness-of-a-Function"><span class="nav-number">2.</span> <span class="nav-text">Goodness of a Function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Find-the-Best-Function"><span class="nav-number">3.</span> <span class="nav-text">Find the Best Function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Comparison"><span class="nav-number">4.</span> <span class="nav-text">Comparison</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#💡思考：为什么逻辑回归不能采用平方损失函数，而采用交叉熵损失函数？"><span class="nav-number">4.1.</span> <span class="nav-text">💡思考：为什么逻辑回归不能采用平方损失函数，而采用交叉熵损失函数？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-Class-Classification-多分类问题"><span class="nav-number">5.</span> <span class="nav-text">Multi-Class Classification 多分类问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Limitation-of-LR-逻辑回归的局限性"><span class="nav-number">6.</span> <span class="nav-text">Limitation of LR 逻辑回归的局限性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#特征变换-Feature-Transformation"><span class="nav-number">6.1.</span> <span class="nav-text">特征变换 Feature Transformation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#生成模型／判别模型"><span class="nav-number">7.</span> <span class="nav-text">生成模型／判别模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#以朴素贝叶斯为例，对两类模型进行说明"><span class="nav-number">7.1.</span> <span class="nav-text">以朴素贝叶斯为例，对两类模型进行说明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#讨论-Generative-v-s-Discriminative"><span class="nav-number">7.2.</span> <span class="nav-text">讨论 Generative v.s. Discriminative</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#极大似然估计MLE-最大后验概率MAP"><span class="nav-number">8.</span> <span class="nav-text">极大似然估计MLE /最大后验概率MAP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#交叉熵与平方损失的不同"><span class="nav-number">9.</span> <span class="nav-text">交叉熵与平方损失的不同</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yeeex</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
